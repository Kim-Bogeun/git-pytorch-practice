{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSgc3B09PDmd",
        "outputId": "100d29c9-05d3-4132-aa16-93b9646fc557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading train-images-idx3-ubyte.gz from https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz ...\n",
            "Downloading train-labels-idx1-ubyte.gz from https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz ...\n",
            "Downloading t10k-images-idx3-ubyte.gz from https://storage.googleapis.com/cvdf-datasets/mnist/t10k-images-idx3-ubyte.gz ...\n",
            "Downloading t10k-labels-idx1-ubyte.gz from https://storage.googleapis.com/cvdf-datasets/mnist/t10k-labels-idx1-ubyte.gz ...\n",
            "Training set: (60000, 784) (60000, 10)\n",
            "Test set    : (10000, 784) (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "#### DO NOT MODIFY THIS CELL ####\n",
        "#################################\n",
        "\n",
        "import os, gzip, urllib.request, warnings\n",
        "import numpy as np\n",
        "\n",
        "FILES = {\n",
        "    \"train-images-idx3-ubyte.gz\": \"X_train\",\n",
        "    \"train-labels-idx1-ubyte.gz\": \"y_train\",\n",
        "    \"t10k-images-idx3-ubyte.gz\" : \"X_test\",\n",
        "    \"t10k-labels-idx1-ubyte.gz\" : \"y_test\",\n",
        "}\n",
        "\n",
        "MIRRORS = [\n",
        "    \"https://storage.googleapis.com/cvdf-datasets/mnist/\",\n",
        "    \"https://yann.lecun.com/exdb/mnist/\",\n",
        "]\n",
        "\n",
        "def download_mnist():\n",
        "    for fname in FILES:\n",
        "        if os.path.exists(fname):\n",
        "            continue\n",
        "        for base in MIRRORS:\n",
        "            url = base + fname\n",
        "            try:\n",
        "                print(f\"Downloading {fname} from {url} ...\")\n",
        "                urllib.request.urlretrieve(url, fname)\n",
        "                break\n",
        "            except urllib.error.HTTPError:\n",
        "                warnings.warn(f\"{url} 404 – trying next mirror…\")\n",
        "        else:\n",
        "            raise RuntimeError(f\"{fname} could not be downloaded from any mirror.\")\n",
        "\n",
        "\n",
        "def load_idx(filename, offset, shape):\n",
        "    with gzip.open(filename, \"rb\") as f:\n",
        "        return np.frombuffer(f.read(), np.uint8, offset=offset).reshape(shape)\n",
        "\n",
        "\n",
        "try:\n",
        "    download_mnist()\n",
        "    # 28×28 이미지 → 784 벡터\n",
        "    X_train = load_idx(\"train-images-idx3-ubyte.gz\", 16, (-1, 28*28))\n",
        "    y_train = load_idx(\"train-labels-idx1-ubyte.gz\", 8,  (-1,))\n",
        "    X_test  = load_idx(\"t10k-images-idx3-ubyte.gz\",  16, (-1, 28*28))\n",
        "    y_test  = load_idx(\"t10k-labels-idx1-ubyte.gz\",  8,  (-1,))\n",
        "except Exception as e:\n",
        "    warnings.warn(f\"MNIST download failed ({e}). Falling back to openml…\")\n",
        "    from sklearn.datasets import fetch_openml\n",
        "    mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
        "    X = mnist[\"data\"].astype(np.float32) / 255.0\n",
        "    y = mnist[\"target\"].astype(np.int8)\n",
        "    X_train, X_test = X[:60000], X[60000:]\n",
        "    y_train, y_test = y[:60000], y[60000:]\n",
        "\n",
        "X_train = X_train.astype(np.float32) / 255.0\n",
        "X_test  = X_test.astype(np.float32) / 255.0\n",
        "\n",
        "num_classes = 10\n",
        "Y_train = np.eye(num_classes)[y_train]\n",
        "Y_test  = np.eye(num_classes)[y_test]\n",
        "\n",
        "print(\"Training set:\", X_train.shape, Y_train.shape)\n",
        "print(\"Test set    :\", X_test.shape,  Y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(input_dim, hidden_dim, output_dim):\n",
        "    \"\"\"\n",
        "    Question (a)\n",
        "\n",
        "    Initialize the neural network's parameters:\n",
        "    - W1: weight matrix of shape (input_dim, hidden_dim)\n",
        "    - b1: bias vector of shape (hidden_dim,)\n",
        "    - W2: weight matrix of shape (hidden_dim, output_dim)\n",
        "    - b2: bias vector of shape (output_dim,)\n",
        "    Returns a dictionary containing W1, b1, W2, b2.\n",
        "    \"\"\"\n",
        "    ##### YOUR CODE #####\n",
        "\n",
        "    W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
        "    b1 = np.zeros(hidden_dim)\n",
        "    W2 = np.random.randn(hidden_dim, output_dim) * 0.01\n",
        "    b2 = np.zeros(output_dim)\n",
        "\n",
        "    #####################\n",
        "    # Tip: When initializing W, use np.random.randn for random initialization and multiply by a small factor (e.g., 0.01)\n",
        "\n",
        "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "pCMIyJ6HPQwZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "    \"\"\"\n",
        "    Question (b)\n",
        "\n",
        "    Compute the softmax of each row of the input array.\n",
        "    logits: NumPy array of shape (N, K) or (K,) representing raw scores.\n",
        "    Returns: NumPy array of same shape, with softmax probabilities.\n",
        "    \"\"\"\n",
        "    ##### YOUR CODE #####\n",
        "\n",
        "    if logits.ndim == 1:\n",
        "        z = logits - np.max(logits)\n",
        "        exp_z = np.exp(z)\n",
        "        probs = exp_z / np.sum(exp_z)\n",
        "\n",
        "    elif logits.ndim == 2:\n",
        "        z = logits - np.max(logits, axis=1, keepdims=True)\n",
        "        exp_z = np.exp(z)\n",
        "        probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    ####################\n",
        "    # Tip: for numeric stability, subtract the max logit from each logit before exponentiating.\n",
        "\n",
        "    return probs\n"
      ],
      "metadata": {
        "id": "ncFgbDBIPU3m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X, params):\n",
        "    \"\"\"\n",
        "    Question (c)\n",
        "\n",
        "    Perform the forward pass.\n",
        "    X: input batch of shape (N, D)\n",
        "    params: dictionary containing W1, b1, W2, b2\n",
        "    Returns:\n",
        "      Y_hat: softmax output probabilities (shape (N, K))\n",
        "      cache: dictionary of intermediate values (z1, a1, z2) for use in backprop.\n",
        "    \"\"\"\n",
        "    W1, b1, W2, b2 = params[\"W1\"], params[\"b1\"], params[\"W2\"], params[\"b2\"]\n",
        "\n",
        "    ##### YOUR CODE #####\n",
        "\n",
        "    z1 = X @ W1 + b1\n",
        "\n",
        "    a1 = np.maximum(0, z1)\n",
        "\n",
        "    z2 = a1 @ W2 + b2\n",
        "\n",
        "    Y_hat = softmax(z2)\n",
        "\n",
        "    ####################\n",
        "\n",
        "    # Store intermediate values in cache for backward pass\n",
        "    cache = {\"X\": X, \"z1\": z1, \"a1\": a1, \"z2\": z2, \"Y_hat\": Y_hat}\n",
        "    return Y_hat, cache\n"
      ],
      "metadata": {
        "id": "FY6DRtPbPW2z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(Y_hat, Y_true, eps = 1e-15):\n",
        "    \"\"\"\n",
        "    Question (d)\n",
        "\n",
        "    Compute the average cross-entropy loss.\n",
        "    Y_hat: predicted probabilities, shape (N, K)\n",
        "    Y_true: true labels (one-hot vectors or class indices), shape (N, K) or (N,)\n",
        "    Returns: scalar loss (average cross-entropy over N examples).\n",
        "    \"\"\"\n",
        "\n",
        "    # Tip: if Y_true is one-hot, multiply element-wise with log(Y_hat) and sum.\n",
        "    # If Y_true is label indices, use those to index into Y_hat.\n",
        "    # Add a small value (e.g., 1e-15) to Y_hat before taking log to avoid log(0).\n",
        "\n",
        "    ##### YOUR CODE #####\n",
        "    n = Y_hat.shape[0]\n",
        "\n",
        "    if Y_true.ndim == 2:\n",
        "        log_probs = np.log(Y_hat + eps)\n",
        "        cross_entropy = -np.sum(Y_true * log_probs, axis=1)\n",
        "\n",
        "        loss = np.mean(cross_entropy)\n",
        "\n",
        "\n",
        "    elif Y_true.ndim == 1:\n",
        "        log_probs = np.log(Y_hat[np.arange(n), Y_true] + eps)\n",
        "        cross_entropy = -log_probs\n",
        "\n",
        "        loss = np.mean(cross_entropy)\n",
        "\n",
        "    #####################\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "CERYtynYPYq7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(X, Y_true, params, cache):\n",
        "    \"\"\"\n",
        "    Question (e)\n",
        "\n",
        "    Perform the backward pass to compute gradients.\n",
        "    X: input batch, shape (N, D)\n",
        "    Y_true: true labels (one-hot vectors or class indices), shape (N, K) or (N,)\n",
        "    params: dictionary of parameters (W1, b1, W2, b2)\n",
        "    cache: dictionary of intermediate values from forward pass\n",
        "    Returns: dictionary of gradients dW1, db1, dW2, db2.\n",
        "    \"\"\"\n",
        "    W1, b1, W2, b2 = params[\"W1\"], params[\"b1\"], params[\"W2\"], params[\"b2\"]\n",
        "    z1, a1, Y_hat = cache[\"z1\"], cache[\"a1\"], cache[\"Y_hat\"]\n",
        "    N = X.shape[0]\n",
        "\n",
        "    if Y_true.ndim == 1: # label -> onehot 변환\n",
        "        Y_true_onehot = np.zeros_like(Y_hat)\n",
        "        Y_true_onehot[np.arange(N), Y_true] = 1\n",
        "    else:\n",
        "        Y_true_onehot = Y_true\n",
        "\n",
        "    ##### YOUR CODE #####\n",
        "    dZ2 = (Y_hat - Y_true_onehot) / N\n",
        "\n",
        "    dW2 = a1.T @ dZ2\n",
        "    db2 = np.sum(dZ2, axis=0)\n",
        "\n",
        "    dA1 = dZ2 @ W2.T\n",
        "\n",
        "    dZ1 = dA1.copy()\n",
        "    dZ1[z1 <= 0] = 0\n",
        "\n",
        "    dW1 = X.T @ dZ1\n",
        "    db1 = np.sum(dZ1, axis=0)\n",
        "\n",
        "    #####################\n",
        "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
        "    return grads"
      ],
      "metadata": {
        "id": "uZrz8GZZPab1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question (f)\n",
        "\"\"\"\n",
        "\n",
        "# ---------------- Hyper-parameters ----------------\n",
        "# 필요한 하이퍼파라미터들을 자유롭게 추가/변경하세요\n",
        "\n",
        "learning_rate = 0.03\n",
        "num_epochs    = 30\n",
        "hidden_dim    = 64     # number of hidden units\n",
        "\n",
        "batch_size = 128\n",
        "# ---------------------------------------------------\n",
        "\n",
        "# Initialize parameters\n",
        "params = initialize_parameters(X_train.shape[1], hidden_dim, 10)\n",
        "\n",
        "N = X_train.shape[0]   # 60 000 examples\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    ##### YOUR CODE #####\n",
        "    idx = np.random.permutation(N)\n",
        "    X_train_shuffled = X_train[idx]\n",
        "    Y_train_shuffled = Y_train[idx]\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(0, N, batch_size):\n",
        "        X_batch = X_train_shuffled[i:i+batch_size]\n",
        "        Y_batch = Y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "        Y_hat, cache = forward(X_batch, params)\n",
        "        loss = compute_loss(Y_hat, Y_batch)\n",
        "        grads = backward(X_batch, Y_batch, params, cache)\n",
        "\n",
        "        params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
        "        params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
        "        params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
        "        params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
        "\n",
        "        epoch_loss += loss * X_batch.shape[0]\n",
        "\n",
        "    epoch_loss /= N\n",
        "    print(f\"Epoch {epoch}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    #####################\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TVzi1x8PgV0",
        "outputId": "87804b5e-7d64-45bf-c7a6-d85048bb44f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 1.6103\n",
            "Epoch 2/30, Loss: 0.5393\n",
            "Epoch 3/30, Loss: 0.3998\n",
            "Epoch 4/30, Loss: 0.3511\n",
            "Epoch 5/30, Loss: 0.3232\n",
            "Epoch 6/30, Loss: 0.3029\n",
            "Epoch 7/30, Loss: 0.2869\n",
            "Epoch 8/30, Loss: 0.2728\n",
            "Epoch 9/30, Loss: 0.2603\n",
            "Epoch 10/30, Loss: 0.2491\n",
            "Epoch 11/30, Loss: 0.2391\n",
            "Epoch 12/30, Loss: 0.2301\n",
            "Epoch 13/30, Loss: 0.2216\n",
            "Epoch 14/30, Loss: 0.2138\n",
            "Epoch 15/30, Loss: 0.2066\n",
            "Epoch 16/30, Loss: 0.1998\n",
            "Epoch 17/30, Loss: 0.1934\n",
            "Epoch 18/30, Loss: 0.1876\n",
            "Epoch 19/30, Loss: 0.1819\n",
            "Epoch 20/30, Loss: 0.1766\n",
            "Epoch 21/30, Loss: 0.1716\n",
            "Epoch 22/30, Loss: 0.1668\n",
            "Epoch 23/30, Loss: 0.1623\n",
            "Epoch 24/30, Loss: 0.1581\n",
            "Epoch 25/30, Loss: 0.1540\n",
            "Epoch 26/30, Loss: 0.1502\n",
            "Epoch 27/30, Loss: 0.1464\n",
            "Epoch 28/30, Loss: 0.1429\n",
            "Epoch 29/30, Loss: 0.1394\n",
            "Epoch 30/30, Loss: 0.1361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question (g)\n",
        "\"\"\"\n",
        "# Evaluate on test set\n",
        "\n",
        "Y_hat_test, _ = forward(X_test, params)\n",
        "test_predictions = np.argmax(Y_hat_test, axis=1)\n",
        "test_targets = y_test  # original labels\n",
        "test_accuracy = np.mean(test_predictions == test_targets) * 100\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "###################################################\n",
        "##### Important: Clearly state the test accuracy values produced by your code.\n",
        "##### Your test accuracy: 95.92%\n",
        "###################################################"
      ],
      "metadata": {
        "id": "v7p-Jk0ZPjRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e540a4-1b3c-4dc9-b563-d1ccd2f483b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 95.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UfRQDsGH0NHO"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}